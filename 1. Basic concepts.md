# Challenges with Hadoop framework
- We need to code al lot
- Difficult to adapt ny non-tech or non-coders
- Difficult process for Ad Hoc queries

# Hive Basics
- It is a data warehousing service
- Built on top of Hadoop
- It uses HDFS file system
- It is SQL like framework &has it's own query language known as HQL
- It converts HQL query into MapReduce code

# Hive Architecture and Components

- The Hive architecture consists of several components that work together to enable data processing and analysis. These components are as follows:

## 1. Hive Metastore
The Hive Metastore is a central repository that stores metadata about Hive tables and partitions. It includes information such as the table name, column names, data types, and storage location of the data. The Hive Metastore can use a variety of databases as its backend, including MySQL, Oracle, and PostgreSQL.

## 2. Hive Driver
The Hive Driver is the main component of Hive that coordinates and executes Hive queries. It receives queries from the user, compiles them into MapReduce jobs, and sends them to the Hadoop cluster for execution. The Hive Driver also communicates with the Hive Metastore to retrieve metadata about the tables and partitions being queried.

## 3. Query Compiler
The Query Compiler is responsible for translating HiveQL queries into MapReduce jobs that can be executed on the Hadoop cluster. It analyzes the structure of the query and generates an execution plan that specifies the sequence of MapReduce jobs that need to be executed to process the query.

## 4. Execution Engine
The Execution Engine is responsible for executing the MapReduce jobs generated by the Query Compiler. It communicates with the Hadoop Distributed File System (HDFS) to read and write data, and it coordinates the execution of the MapReduce jobs across the Hadoop cluster.

## 5. Hadoop Cluster
The Hadoop Cluster is the underlying infrastructure that Hive runs on. It consists of a collection of commodity hardware nodes that work together to store and process large amounts of data. Hive uses Hadoop's distributed file system (HDFS) to store data, and it uses Hadoop's MapReduce framework to execute queries in parallel across the cluster.

## 6. Hadoop Input/Output Formats
Hadoop Input/Output Formats are used by Hive to read and write data to and from HDFS. These formats define how data is serialized and deserialized when it is read from or written to HDFS. Hive supports a variety of Input/Output Formats, including TextFile, SequenceFile, and RCFile.

## 7. User Interface
Hive provides a command-line interface (CLI) and a web-based graphical user interface (GUI) for users to interact with Hive.
