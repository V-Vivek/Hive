# Hive commands

## Create Managed table
```
CREATE TABLE employee
(
  emp_id INT,
  emp_name STRING,
  emp_dept STRING
)
ROW FORMAT DELIMITED
FIELDS TERMINATED BY ',';
```
#### FIELDS TERMINATED BY ',' clause 
- It specifies that each field in the data is separated by a comma (',') character. 
- This is a common format for data that is stored in CSV (Comma-Separated Values) files.

#### ROW FORMAT DELIMITED clause
- It specifies that each row in the data is separated by a delimiter character, which is specified by the "FIELDS TERMINATED BY" or "LINES TERMINATED BY" clauses. 
- If these clauses are not specified, Hive uses default delimiters ('\n' for lines and '\t' for fields).

## Load data from local
```
LOAD DATA LOCAL INPATH 'file://path_to_data_directory/data.csv' INTO TABLE table_name;
```

## Display column name
```
SET hive.cli.print.header = true;
```

## Load data from HDFS
```
LOAD DATA INPATH '/path_to_HDFS_data_directory/data.csv' INTO TABLE table_name;
```

## Create External table
- Note that we need to specify the location of data while creating the table itself.
- Hence, no need to load the data manually.
- Also note that in LOCATION we have specified the directory path instead of file name. So that if there are multiple files in that directory with same schema all of them will be loaded in the table.
- Also in future if new filesa re added in that directory they will also be auto loaded in the table.
- But make sure that the schema of all the data files in the directory should match with the schema of table.
```
CREATE EXTERNAL TABLE employee
(
  emp_id INT,
  emp_name STRING,
  emp_dept STRING
)
ROW FORMAT DELIMITED
FIELDS TERMINATED BY ','
LOCATION '/path_to_HDFS_data_directory/';
```


## Working with Array data types
```
CREATE TABLE employee
(
  id INT,
  name STRING,
  skills ARRAY<STRING>
)
ROW FORMAT DELIMITED
FIELDS TERMINATED BY ','
COLLECTION ITEMS TERMINATED BY ':';
```
```
LOAD DATA LOCAL INPATH 'file:///config/workspace/array_data.csv' INTO TABLE employee;
```

## Indexing in Hive Array data type
```
SELECT id, name, skills[0] AS prime_skill
FROM employee;
```

## To create a copy of table
```
CREATE TABLE mytable_copy AS
SELECT * FROM mytable;
```

## To skip header or skip first 'n' rows of data
- Consider a data which has headers in it. So if we don't want to insert those header names in our Hive table.
- We use ```TBLPROPERTIES ("skip.header.line.count"="1")```. Note that the no. of rows to be skipped is passed as a string & not as an integer.
### Data
```
id,name,age
1,John,25
2,Jane,30
3,Bob,35
```
```
CREATE TABLE my_table 
(
  employee_id INT,
  employee_name STRING,
  employee_age INT
)
ROW FORMAT DELIMITED
FIELDS TERMINATED BY ','
TBLPROPERTIES ("skip.header.line.count"="1");
```

## Hive properties to set specific number for reducers
- By default, Hive will use a single reducer for each MapReduce job, which can lead to performance issues when processing large amounts of data.
- Reducers are responsible for aggregating and summarizing the output of map tasks and producing the final output of the job.

### In order to change the average load for a reducer (in bytes):
- By setting hive.exec.reducers.bytes.per.reducer to an appropriate value, you can increase the number of reducers used by Hive and improve performance.
- For example, to set 256 MB, you would use the following command in Hive:
```
SET hive.exec.reducers.bytes.per.reducer=268435456;
```

### In order to limit the maximum number of reducers:
- It's important to note that setting this property too low can result in slower query performance or even query failure if there are not enough reducers to handle the data. 
- Conversely, setting this property too high can also result in slower performance due to increased overhead and contention for resources.
- For example, to set max 10 reducers, you would use the following command in Hive:
```
SET hive.exec.reducers.max=10;
```

### In order to set a constant number of reducers:
```
SET mapreduce.job.reduces=4;
```
