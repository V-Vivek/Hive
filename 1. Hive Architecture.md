# Challenges with Hadoop framework
- We need to code a lot
- Difficult to adapt by non-tech or non-coders
- Difficult process for Ad Hoc queries

# Hive Basics
- It is a data warehousing service
- Built on top of Hadoop
- It uses HDFS file system
- It is SQL like framework & has it's own query language known as HQL
- It converts HQL query into MapReduce code

# Hive Architecture
![image](https://user-images.githubusercontent.com/117569148/224492472-08b0e2ac-e001-447f-b020-6c29c75a80cf.png)

# Hive Clients
- Hive clients are the tools that users use to interact with Hive. Hive provides several client interfaces, including:

## Thrift
- Thrift is a lightweight, cross-platform remote procedure call (RPC) framework that is used by Hive to provide a client interface for various programming languages such as Java, Python, C++, and others.

## JDBC
- JDBC (Java Database Connectivity) is a standard Java API that enables Java programs to interact with databases. 
- Hive provides a JDBC driver that allows Java programs to interact with Hive.

## ODBC
- ODBC (Open Database Connectivity) is a standard interface for accessing data from a variety of database management systems. 
- Hive provides an ODBC driver that allows applications that support ODBC to interact with Hive.

# Hive Services
- Hive consists of several services that work together to process user queries. The main Hive services are:

## HiveServer2
- HiveServer2 is the successor of HiveServer1.
- HiveServer1 does not handle concurrent requests from more than one client due to which it was replaced by HiveServer2.
- HiveServer2 is the main service that provides a Thrift interface for clients to interact with Hive.
- It receives queries from clients, compiles them, optimizes them and executes them on the Hadoop cluster.

## Hive Driver
- The Hive driver receives the HiveQL statements submitted by the user through the command shell. 
- It creates the session handles for the query and sends the query to the compiler.

## Hive Compiler
- Hive compiler parses the query. 
- It performs semantic analysis and type-checking on the different query blocks and query expressions by using the metadata stored in metastore.
- It generates an execution plan.
- The execution plan created by the compiler is the ***DAG(Directed Acyclic Graph)***, where each stage is a map/reduce job, operation on HDFS, a metadata operation.


## Optimizer
- Optimizer performs the transformation operations on the execution plan and splits the task to improve efficiency and scalability.

## Execution Engine
- Execution engine, after the compilation and optimization steps, executes the execution plan created by the compiler in order of their dependencies using Hadoop.

## Metastore
- Metastore is a central repository that stores the metadata information about the structure of tables and partitions, including column and column type information.
- It also stores information of serializer and deserializer, required for the read/write operation and HDFS files where data is stored. 
- This metastore is generally a relational database.
- Metastore provides a Thrift interface for querying and manipulating Hive metadata.


